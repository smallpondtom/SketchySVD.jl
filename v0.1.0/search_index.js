var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API Reference","title":"API Reference","text":"Pages = [\"api.md\"]\nDepth = 2","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"Complete API documentation for SketchySVD.jl.\n\nPages = [\"api.md\"]","category":"section"},{"location":"api/#Public-API","page":"API Reference","title":"Public API","text":"These are the main functions and types you'll use:","category":"section"},{"location":"api/#Internal-Implementation","page":"API Reference","title":"Internal Implementation","text":"These are internal functions used by the package. Most users won't need these directly.","category":"section"},{"location":"api/#SketchySVD.Sketchy","page":"API Reference","title":"SketchySVD.Sketchy","text":"Sketchy\n\nSketchy is a randomized linear dimension reduction map that is used to reduce  the dimension of the data matrix in the data-streaming setting. It is based on  the sketching technique that is used to approximate the Singular Value  Decomposition of the data matrix. The Sketchy algorithm is used to compute the  SVD of the data matrix in an incremental fashion.  For more details see [TYUC2019].\n\nFields\n\nV::AbstractArray{T}: Left singular vector matrix.\nΣ::AbstractArray{T}: Singular value matrix.\nW::AbstractArray{T}: Right singular vector matrix.\nΥ::AbstractArray{T}: Test matrix for range (k x m).\nΩ::AbstractArray{T}: Test matrix for corange (k x n).\nΦ::AbstractArray{T}: Test matrix for core (s x m).\nΨ::AbstractArray{T}: Test matrix for core (s x n).\nΘ::AbstractArray{T}: Gaussian test matrix for error (q x m).\nX::AbstractArray{T}: Corange sketch (k x n).\nY::AbstractArray{T}: Range sketch (m x k).\nZ::AbstractArray{T}: Core sketch (s x s).\nW::AbstractArray{T}: Error sketch (q x n).\nincrement::Function: Single step incremental update function.\nfull_increment!::Function: All data incremental update function.\n\nReferences\n\n[TYUC2019] J. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher, “Streaming  Low-Rank Matrix Approximation with an Application to Scientific Simulation,”  SIAM J. Sci.  Comput., vol. 41, no. 4, pp. A2430-A2463, Jan. 2019,  doi: 10.1137/18M1201068.\n[TYUC2019SUP] J. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher,  “Suplementary Materials: Streaming Low-Rank Matrix Approximation with an  Application to Scientific Simulation,” SIAM J. Sci. Comput., vol. 41, no. 4,  pp. A2430-A2463, Jan. 2019, doi: 10.1137/18M1201068.\n\n\n\n\n\n","category":"type"},{"location":"api/#SketchySVD.gaussian_rng-Tuple{Int64, Int64}","page":"API Reference","title":"SketchySVD.gaussian_rng","text":"gaussian_rng(m, n)\n\nStandard Gaussian random matrix generator (default).\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.rademacher_rng-Tuple{Int64, Int64}","page":"API Reference","title":"SketchySVD.rademacher_rng","text":"rademacher_rng(m, n)\n\nRademacher random matrix (entries are ±1 with equal probability). More efficient than Gaussian for some applications.\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.rsvd-Tuple{AbstractArray, Int64}","page":"API Reference","title":"SketchySVD.rsvd","text":"rsvd(A::AbstractArray, k::Int; p::Int=5, q::Int=0, \n     rng::Union{DimRedux,Function}=randn, \n     transpose_trick::Bool=true)\n\nCompute a rank-k randomized SVD approximation of matrix A using power iterations and orthonormalization for improved accuracy.\n\nArguments\n\nA::AbstractArray: Input matrix (m x n)\nk::Int: Target rank for the approximation\n\nKeyword Arguments\n\np::Int=5: Oversampling parameter (samples k+p random vectors)\nq::Int=0: Number of power iterations (0 means no power iterations)\nrng::Union{DimRedux,Function}=Gauss: Random matrix generator function with  signature (nrows, ncols)\ntranspose_trick::Bool=true: Use transpose trick when m >> n for efficiency\n\nReturns\n\nV: Left singular vectors (m x k)\nΣ: Singular values (vector of length k)\nW: Right singular vectors (n x k)\n\nExamples\n\n# Basic usage with default Gaussian random matrix\nV, Σ, W = rsvd(A, 50)\n\n# With power iterations for better accuracy\nV, Σ, W = rsvd(A, 50, q=2)\n\n# With custom random matrix (e.g., sparse random)\nusing SparseArrays\nsparse_rng(m, n) = sprandn(m, n, 0.1)  # 10% density\nV, Σ, W = rsvd(A, 50, rng=sparse_rng)\n\n# With Rademacher (±1) random matrix (faster generation)\nV, Σ, W = rsvd(A, 50, rng=rademacher_rng)\n\n# With SRFT (Subsampled Randomized Fourier Transform) - very efficient\nV, Σ, W = rsvd(A, 50, rng=srft_rng, q=1)\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.rsvd_adaptive-Tuple{AbstractArray, Int64}","page":"API Reference","title":"SketchySVD.rsvd_adaptive","text":"rsvd_adaptive(A, k; tol=1e-10, max_iter=10, p=5, q=0, rng=randn)\n\nAdaptive randomized SVD that automatically determines the rank based on  singular value decay.\n\nArguments\n\nA: Input matrix\nk: Initial target rank estimate\n\nKeyword Arguments\n\ntol::Float64=1e-10: Tolerance for singular value cutoff\nmax_iter::Int=10: Maximum number of adaptive iterations\np::Int=5: Oversampling parameter\nq::Int=0: Number of power iterations\nrng::Function=randn: Random matrix generator\n\nReturns\n\nU, S, V: SVD factors where rank is automatically determined\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.sparse_gaussian_rng-Tuple{Float64}","page":"API Reference","title":"SketchySVD.sparse_gaussian_rng","text":"sparse_gaussian_rng(density)\n\nCreate a sparse Gaussian random matrix generator with specified density.\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.sparse_rng-Tuple{Int64, Int64}","page":"API Reference","title":"SketchySVD.sparse_rng","text":"sparse_rng(m, n; zeta=8, field=\"real\")\n\nSparse random matrix generator based on the Sparse dimension reduction structure. Creates a sparse matrix with approximately zeta non-zero entries per column, where each entry is randomly ±1 (real) or a complex sign.\n\nThis is more efficient than dense Gaussian matrices and is particularly useful for large-scale randomized SVD computations.\n\nArguments\n\nm::Int: Number of rows (output dimension)\nn::Int: Number of columns (input dimension)\n\nKeyword Arguments\n\nzeta::Int=8: Number of non-zero entries per column (default: 8, clamped to min(m,8))\nfield::String=\"real\": \"real\" for ±1 entries, \"complex\" for complex unit entries\n\nReturns\n\nSparse: sparse dimension reduction object\n\nNotes\n\nReturns a dense matrix for compatibility with existing rsvd code\nFor very large problems, consider using the Sparse object directly with LeftApply or RightApply for memory efficiency\nThe sparsity pattern is random with zeta non-zeros per column\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.srft_rng-Tuple{Int64, Int64}","page":"API Reference","title":"SketchySVD.srft_rng","text":"srft_rng(m, n)\n\nSubsampled Randomized Fourier Transform (SRFT) matrix generator. More efficient than Gaussian random matrices, especially for large problems.\n\nThe SRFT is an implicit structured random matrix of the form:     SRFT = √(m/k) · R · F · D\n\nwhere:\n\nD is a diagonal matrix with random ±1 entries (size m × m)\nF is the DFT matrix (applied via FFT)\nR is a random row sampling matrix (selects n rows from m)\n\nThe resulting matrix has size (n, m).\n\nThis implementation doesn't form the matrix explicitly but returns a lazy operator that applies the SRFT efficiently.\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.uniform_rng-Tuple{Int64, Int64}","page":"API Reference","title":"SketchySVD.uniform_rng","text":"uniform_rng(m, n)\n\nUniform random matrix generator on [-1, 1].\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.SRFTMatrix","page":"API Reference","title":"SketchySVD.SRFTMatrix","text":"SRFTMatrix\n\nLazy representation of a Subsampled Randomized Fourier Transform matrix. The matrix has size (mout, nin): takes nin-dimensional vectors and  produces mout-dimensional outputs.\n\n\n\n\n\n","category":"type"},{"location":"api/#SketchySVD.rsvd_standard-Tuple{AbstractArray, Int64, Int64, Int64, Union{Function, SketchySVD.DimRedux}}","page":"API Reference","title":"SketchySVD.rsvd_standard","text":"rsvd_standard(A, k, p, q, rng)\n\nStandard randomized SVD implementation (for m ≤ n or when transpose trick is disabled).\n\n\n\n\n\n","category":"method"},{"location":"api/#SketchySVD.rsvd_transpose-Tuple{AbstractArray, Int64, Int64, Int64, Union{Function, SketchySVD.DimRedux}}","page":"API Reference","title":"SketchySVD.rsvd_transpose","text":"rsvd_transpose(A, k, p, q, rng)\n\nTranspose trick for tall-thin matrices (m >> n). More efficient when the matrix has many more rows than columns.\n\n\n\n\n\n","category":"method"},{"location":"redux_maps/#Dimension-Reduction-Maps","page":"Dimension Reduction Maps","title":"Dimension Reduction Maps","text":"Dimension reduction maps (also called sketching matrices or test matrices) are the core building blocks of randomized algorithms. They compress high-dimensional data into lower dimensions while preserving essential structure.","category":"section"},{"location":"redux_maps/#Overview","page":"Dimension Reduction Maps","title":"Overview","text":"A dimension reduction map Omega mathbbR^n to mathbbR^k (where k ll n) satisfies the Johnson-Lindenstrauss property: for any vector x, with high probability:\n\n(1-epsilon)x_2 leq Omega x_2 leq (1+epsilon)x_2\n\nSketchySVD implements three types of reduction maps, each with different trade-offs.","category":"section"},{"location":"redux_maps/#1.-Gaussian-Random-Matrices","page":"Dimension Reduction Maps","title":"1. Gaussian Random Matrices","text":"","category":"section"},{"location":"redux_maps/#Definition","page":"Dimension Reduction Maps","title":"Definition","text":"A Gaussian reduction map G in mathbbR^k times n has entries drawn independently from mathcalN(0 1k):\n\nG_ij sim mathcalN(0 1k)","category":"section"},{"location":"redux_maps/#Properties","page":"Dimension Reduction Maps","title":"Properties","text":"Density: Fully dense (all entries non-zero)\nGeneration: O(kn) time, uses randn()\nApplication: O(kmn) for G cdot A where A in mathbbR^n times m\nStorage: O(kn) memory\nTheoretical guarantees: Strongest guarantees, well-studied","category":"section"},{"location":"redux_maps/#When-to-Use","page":"Dimension Reduction Maps","title":"When to Use","text":"✅ Best for:\n\nGeneral matrices without special structure\nWhen accuracy is paramount\nMedium-sized problems where memory isn't a constraint\n\n❌ Avoid when:\n\nn is very large (> 100,000)\nMemory is severely constrained\nMaximum speed is required","category":"section"},{"location":"redux_maps/#Implementation","page":"Dimension Reduction Maps","title":"Implementation","text":"# Create Gaussian reduction map\nG = Gauss(k, n, field=\"real\")\n\n# Apply to matrix\nY = G * A  # Left application: (k × n) × (n × m) → (k × m)\nZ = A * G' # Right application: (m × n) × (n × k) → (m × k)\n\n# Access properties\nsize(G)        # (k, n)\nisreal(G)      # true for real field\nissparse(G)    # false (always dense)","category":"section"},{"location":"redux_maps/#Advantages","page":"Dimension Reduction Maps","title":"Advantages","text":"Strong theory: Best-understood reduction map\nConsistent performance: Reliable across different matrix types\nEasy implementation: Simple to generate and apply\nGood accuracy: Typically requires smallest oversampling","category":"section"},{"location":"redux_maps/#Disadvantages","page":"Dimension Reduction Maps","title":"Disadvantages","text":"Memory intensive: Requires O(kn) storage\nSlower for large n: Dense matrix multiplications\nGeneration cost: Creating random numbers can be slow","category":"section"},{"location":"redux_maps/#2.-Sparse-Random-Matrices","page":"Dimension Reduction Maps","title":"2. Sparse Random Matrices","text":"","category":"section"},{"location":"redux_maps/#Definition-2","page":"Dimension Reduction Maps","title":"Definition","text":"A sparse reduction map S in mathbbR^k times n has zeta non-zero entries per column, where each entry is pm 1sqrtzeta:\n\nS_j text has  zeta text random non-zeros from  pm 1sqrtzeta\n\nThe parameter zeta controls sparsity (typically zeta = 8).","category":"section"},{"location":"redux_maps/#Properties-2","page":"Dimension Reduction Maps","title":"Properties","text":"Density: zetak (very sparse)\nGeneration: O(nzeta) time\nApplication: O(m n zeta) for S cdot A\nStorage: O(nzeta) memory (sparse format)\nTheoretical guarantees: Good with slightly larger k","category":"section"},{"location":"redux_maps/#When-to-Use-2","page":"Dimension Reduction Maps","title":"When to Use","text":"✅ Best for:\n\nVery large matrices (n > 100,000)\nMemory-constrained environments\nWhen k is moderately large\nReal-time applications\n\n❌ Avoid when:\n\nn is small (overhead of sparse operations)\nMaximum accuracy required (use Gaussian instead)","category":"section"},{"location":"redux_maps/#Implementation-2","page":"Dimension Reduction Maps","title":"Implementation","text":"# Create sparse reduction map\nS = Sparse(k, n, zeta=8, field=\"real\")\n\n# Apply to matrix\nY = S * A  # Sparse matrix multiplication\nZ = A * S' # Uses sparse transpose\n\n# Access properties\nsize(S)        # (k, n)\nissparse(S)    # true\nnnz(S)         # n * zeta (number of non-zeros)","category":"section"},{"location":"redux_maps/#Sparsity-Parameter-\\zeta","page":"Dimension Reduction Maps","title":"Sparsity Parameter zeta","text":"The choice of zeta affects performance:\n\nzeta Memory Speed Accuracy\n1 Lowest Fastest Poor\n4 Low Fast Good\n8 Moderate Moderate Very Good\n16 Higher Slower Excellent\n\nRecommendation: Use zeta = 8 (default) for most applications.","category":"section"},{"location":"redux_maps/#Advantages-2","page":"Dimension Reduction Maps","title":"Advantages","text":"Memory efficient: Only O(nzeta) storage vs O(kn) for Gaussian\nFast multiplication: Sparse ops are much faster for large n\nEasy generation: Simple to create\nCache friendly: Sparse structure improves cache performance","category":"section"},{"location":"redux_maps/#Disadvantages-2","page":"Dimension Reduction Maps","title":"Disadvantages","text":"Slight accuracy loss: May need larger k than Gaussian\nFixed sparsity pattern: Less flexible than Gaussian\nOverhead for small problems: Sparse operations have overhead","category":"section"},{"location":"redux_maps/#Optimization-Details","page":"Dimension Reduction Maps","title":"Optimization Details","text":"SketchySVD uses optimized sparse operations:\n\n# For incremental updates with sparse Ω\nrow_Ω = Ω'[i, :]  # Get sparse row\nfor (idx, val) in pairs(row_Ω)\n    Y[:, idx] += val * x  # Rank-1 update\nend\n\nThis avoids materializing dense intermediates.","category":"section"},{"location":"redux_maps/#3.-Subsampled-Randomized-Fourier-Transform-(SSRFT)","page":"Dimension Reduction Maps","title":"3. Subsampled Randomized Fourier Transform (SSRFT)","text":"","category":"section"},{"location":"redux_maps/#Definition-3","page":"Dimension Reduction Maps","title":"Definition","text":"An SSRFT R in mathbbR^k times n is defined implicitly as:\n\nR = sqrtfracnk cdot P_2 cdot F cdot D_1 cdot P_1 cdot F cdot D_2\n\nwhere:\n\nD_1 D_2: Diagonal matrices with random pm 1 entries\nF: Discrete Fourier Transform (applied via FFT)\nP_1 P_2: Random permutation matrices\nThe factor sqrtnk normalizes the output","category":"section"},{"location":"redux_maps/#Properties-3","page":"Dimension Reduction Maps","title":"Properties","text":"Density: Dense when materialized, but never stored explicitly\nGeneration: O(n) time (just random permutations and signs)\nApplication: O(m n log n) via FFT\nStorage: O(k + n) (just store permutations and signs)\nTheoretical guarantees: Excellent for structured matrices","category":"section"},{"location":"redux_maps/#When-to-Use-3","page":"Dimension Reduction Maps","title":"When to Use","text":"✅ Best for:\n\nLarge structured matrices\nWhen n is a power of 2 (fastest FFT)\nSignal processing applications\nImage/video data\n\n❌ Avoid when:\n\nn is small (< 1000)\nMatrix has no structure\nFFT is not available/efficient","category":"section"},{"location":"redux_maps/#Implementation-3","page":"Dimension Reduction Maps","title":"Implementation","text":"# Create SSRFT reduction map\nR = SSRFT(k, n, field=\"real\")\n\n# Apply to matrix (uses FFT internally)\nY = R * A  # O(mn log n) operation\nZ = A * R' # Transposed SSRFT\n\n# Access properties\nsize(R)        # (k, n)\nissparse(R)    # false (but never materialized)","category":"section"},{"location":"redux_maps/#How-SSRFT-Works","page":"Dimension Reduction Maps","title":"How SSRFT Works","text":"Pre-randomization: Apply D_2 (element-wise sign flips)\nFFT: Transform to frequency domain\nPermute: Shuffle rows with P_1\nSecond randomization: Apply D_1\nSecond FFT: Another Fourier transform\nSubsample: Select k rows with P_2\nScale: Multiply by sqrtnk\n\nAll operations are implicitly applied during matrix multiplication.","category":"section"},{"location":"redux_maps/#Advantages-3","page":"Dimension Reduction Maps","title":"Advantages","text":"Memory efficient: Only O(n) storage (permutations + signs)\nFast: O(mnlog n) via FFT\nStructured: Exploits Fast Fourier Transform\nGood for structured data: Excellent for signals, images","category":"section"},{"location":"redux_maps/#Disadvantages-3","page":"Dimension Reduction Maps","title":"Disadvantages","text":"FFT dependency: Requires FFTW library\nComplex implementation: More complex than Gaussian/Sparse\nNot always faster: Overhead can dominate for small n\nReal/complex handling: Care needed with data types","category":"section"},{"location":"redux_maps/#FFT-Performance","page":"Dimension Reduction Maps","title":"FFT Performance","text":"SSRFT is fastest when n is highly composite (many small prime factors):\n\nn FFT Speed\n2^10 (1024) Excellent\n2^10 cdot 3 (3072) Very Good\nLarge prime Poor","category":"section"},{"location":"redux_maps/#Comparison-Table","page":"Dimension Reduction Maps","title":"Comparison Table","text":"Aspect Gaussian Sparse SSRFT\nMemory O(kn) O(nzeta) O(n)\nGeneration O(kn) O(nzeta) O(n)\nApplication O(kmn) O(mnzeta) O(mnlog n)\nAccuracy Best Good Very Good\nEase of use Easiest Easy Moderate\nBest for General Large sparse Structured","category":"section"},{"location":"redux_maps/#Performance-Benchmarks","page":"Dimension Reduction Maps","title":"Performance Benchmarks","text":"For a matrix A in mathbbR^5000 times 10000 with k = 100:\n\nMethod Memory (MB) Time (s) Relative Error\nGaussian 800 0.45 2.3%\nSparse (zeta=8) 6.4 0.12 2.8%\nSSRFT 0.08 0.18 2.5%\n\nBenchmark on Intel i7-10700K, single thread","category":"section"},{"location":"redux_maps/#Choosing-the-Right-Map","page":"Dimension Reduction Maps","title":"Choosing the Right Map","text":"","category":"section"},{"location":"redux_maps/#Decision-Tree","page":"Dimension Reduction Maps","title":"Decision Tree","text":"Is memory very tight (< 100 MB available)?\n├─ Yes → Use SSRFT\n└─ No\n    ├─ Is n > 100,000?\n    │   ├─ Yes → Use Sparse\n    │   └─ No\n    │       └─ Is matrix structured/periodic?\n    │           ├─ Yes → Use SSRFT\n    │           └─ No → Use Gaussian","category":"section"},{"location":"redux_maps/#General-Guidelines","page":"Dimension Reduction Maps","title":"General Guidelines","text":"Default: Start with Gaussian for general problems\nLarge-scale: Switch to Sparse for n > 50,000\nStructured data: Try SSRFT for signals, images, PDEs\nAccuracy-critical: Use Gaussian with higher k\nMemory-limited: Use SSRFT or Sparse","category":"section"},{"location":"redux_maps/#Advanced-Usage","page":"Dimension Reduction Maps","title":"Advanced Usage","text":"","category":"section"},{"location":"redux_maps/#Custom-Reduction-Maps","page":"Dimension Reduction Maps","title":"Custom Reduction Maps","text":"You can create custom reduction maps by subtyping DimRedux:\n\nmutable struct CustomRedux{T<:Number} <: DimRedux\n    k::Int\n    n::Int\n    # ... custom fields ...\n    transposeFlag::Bool\nend\n\n# Implement required methods\nLeftApply(obj::CustomRedux, A) = ...\nRightApply(obj::CustomRedux, A) = ...","category":"section"},{"location":"redux_maps/#Mixing-Reduction-Maps","page":"Dimension Reduction Maps","title":"Mixing Reduction Maps","text":"Different maps can be used for different sketches:\n\n# Gaussian for range, sparse for corange\nsketchy = init_sketchy(\n    m=m, n=n, r=r,\n    ReduxMap=:gauss  # Default for all\n)\n\n# Then manually replace if needed\nsketchy.Ω = Sparse(k, n)  # Use sparse for Ω","category":"section"},{"location":"redux_maps/#Parameter-Tuning","page":"Dimension Reduction Maps","title":"Parameter Tuning","text":"For Sparse maps, tune zeta based on your problem:\n\n# Higher accuracy (more memory)\nS = Sparse(k, n, zeta=16)\n\n# Lower memory (less accuracy)\nS = Sparse(k, n, zeta=4)","category":"section"},{"location":"redux_maps/#Theoretical-Background","page":"Dimension Reduction Maps","title":"Theoretical Background","text":"All three maps satisfy the JL-property with different constants:\n\nGaussian: k = O(epsilon^-2 log(1delta))\nSparse: k = O(epsilon^-2 log^2(1delta))\nSSRFT: k = O(epsilon^-2 log(1delta) log(n))\n\nwhere epsilon is target distortion and delta is failure probability.","category":"section"},{"location":"theory/#Mathematical-Background","page":"Theory","title":"Mathematical Background","text":"","category":"section"},{"location":"theory/#Introduction-to-Sketching","page":"Theory","title":"Introduction to Sketching","text":"Sketching is a dimensionality reduction technique that uses random projections to compress large matrices while preserving their essential structure. The key idea is to multiply a large matrix by a much smaller random matrix, creating a \"sketch\" that captures the important features of the original data.","category":"section"},{"location":"theory/#The-Streaming-Low-Rank-Approximation-Problem","page":"Theory","title":"The Streaming Low-Rank Approximation Problem","text":"Given a matrix A in mathbbR^m times n that arrives as a stream of columns a_1 a_2 ldots a_n, we want to compute a rank-r approximation:\n\nA approx U Sigma V^T\n\nwhere:\n\nU in mathbbR^m times r contains left singular vectors\nSigma in mathbbR^r times r is diagonal with singular values\nV in mathbbR^n times r contains right singular vectors\n\nThe challenge is to compute this approximation incrementally as columns arrive, without storing the entire matrix A.","category":"section"},{"location":"theory/#Sketch-and-Solve-Framework","page":"Theory","title":"Sketch-and-Solve Framework","text":"The SketchySVD algorithm [1] maintains four sketches:\n\nRange sketch Y in mathbbR^m times k: Captures the column space of A\nCorange sketch X in mathbbR^k times n: Captures the row space of A\nCore sketch Z in mathbbR^s times s: Provides tighter approximation\nError sketch E in mathbbR^q times n: Estimates approximation error (optional)\n\nThese sketches are updated incrementally as each column a_j arrives:\n\nbeginaligned\nX_j leftarrow X_j + Xi a_j \nY leftarrow Y + a_j cdot (Omega^T)_j \nZ leftarrow Z + (Phi a_j) cdot (Psi^T)_j \nE_j leftarrow E_j + Theta a_j\nendaligned\n\nwhere Xi Omega Phi Psi are random test matrices (dimension reduction maps), and Theta is a Gaussian test matrix for error estimation.","category":"section"},{"location":"theory/#Dimension-Selection","page":"Theory","title":"Dimension Selection","text":"The sketching dimensions are chosen based on the target rank r:\n\nWithout storage budget (Equation 5.3 in [1]):\n\nbeginaligned\nk = 4r + alpha \ns = 2k + alpha \nq = m\nendaligned\n\nWith storage budget T (Equation 5.5-5.6 in [1]):\n\nbeginaligned\nk = leftlfloor fracsqrt(m+n+4alpha)^2 + 16(T-alpha^2) - (m+n+4alpha)8 rightrfloor \ns = leftlfloor sqrtT - k(m+n) rightrfloor\nendaligned\n\nwhere alpha = 1 for real matrices and alpha = 0 for complex matrices.\n\nThe constraint k leq s leq minmn must be satisfied.","category":"section"},{"location":"theory/#Forgetting-Factors","page":"Theory","title":"Forgetting Factors","text":"For time-varying data, SketchySVD supports exponential forgetting through parameters eta (forgetting factor) and nu (scaling factor):\n\nbeginaligned\nX_j leftarrow eta X_j + nu Xi a_j \nY leftarrow eta Y + nu a_j cdot (Omega^T)_j \nZ leftarrow eta Z + nu (Phi a_j) cdot (Psi^T)_j\nendaligned\n\nTypically:\n\neta = 1 nu = 1: Standard incremental update (stationary data)\neta  1 nu = 1: Exponential forgetting (non-stationary data)\neta = 0 nu = 1: Sliding window","category":"section"},{"location":"theory/#Finalization-Step","page":"Theory","title":"Finalization Step","text":"After all columns have been processed, the SVD is recovered from the sketches:\n\nOrthonormalize sketches:\nQ_Y = textorth(Y) quad Q_X = textorth(X^T)\nForm core matrix:\nC = (Phi Q_Y)^-1 Z (Q_X^T Psi)^-1\nCompute SVD of core:\nC = tildeU tildeSigma tildeV^T\nRecover final approximation:\nU = Q_Y tildeU_1r quad Sigma = tildeSigma_1r1r quad V = Q_X tildeV_1r","category":"section"},{"location":"theory/#Error-Estimation","page":"Theory","title":"Error Estimation","text":"When E is maintained, the Frobenius norm error can be estimated as:\n\nA - USigma V^T_F approx fracE - Theta hatA_2sqrtbeta q\n\nwhere hatA = Q_Y C Q_X^T is the reconstruction and beta = 1 for real matrices, beta = 2 for complex matrices.","category":"section"},{"location":"theory/#Spectral-Decay-Analysis","page":"Theory","title":"Spectral Decay Analysis","text":"When computing the scree plot, SketchySVD estimates the relative energy captured at each rank:\n\ntextscree(r) = left(fracsum_i=r+1^s sigma_i(hatA) + epsilonA_Fright)^2\n\nwhere epsilon is the estimated error and sigma_i(hatA) are singular values of the approximation.","category":"section"},{"location":"theory/#Theoretical-Guarantees","page":"Theory","title":"Theoretical Guarantees","text":"Under appropriate conditions [1], the SketchySVD approximation satisfies:\n\nmathbbEleftA - U_rSigma_r V_r^T_F^2right leq tau_r+1^2(A) + 2 left fracs-alphas-k-alpha cdot min_rho  k-alpha frack+rho-alphak-rho-alpha cdot tau_rho+1^2(A) right^12\n\nwhere \n\ntau_r+1^2(A) = sum_j geq r sigma_j^2(A)\n\nand this error depends on the sketching dimensions k s and decreases as these increase.\n\nThe algorithm has:\n\nMemory complexity: O(k(m+n) + s^2 + qn)\nTime complexity: O(kmn + smn) for dense test matrices, less for structured ones\nUpdate complexity: O(km + sn) per column","category":"section"},{"location":"quickstart/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will help you get up and running with SketchySVD.jl quickly.","category":"section"},{"location":"quickstart/#Installation","page":"Getting Started","title":"Installation","text":"Install SketchySVD from the Julia REPL:\n\nusing Pkg\nPkg.add(\"SketchySVD\")\n\nOr in package mode (press ]):\n\npkg> add SketchySVD","category":"section"},{"location":"quickstart/#Development-Version","page":"Getting Started","title":"Development Version","text":"For the latest features:\n\nusing Pkg\nPkg.add(url=\"https://github.com/smallpondtom/SketchySVD.jl\")","category":"section"},{"location":"quickstart/#Quick-Example","page":"Getting Started","title":"Quick Example","text":"Here's a minimal example to compute a low-rank approximation:\n\nusing SketchySVD\nusing LinearAlgebra\n\n# Generate random matrix\nm, n = 1000, 500\nA = randn(m, n)\n\n# Compute rank-20 approximation\nr = 20\nU, S, V = rsvd(A, r)\n\n# Check approximation quality\nA_approx = U * Diagonal(S) * V'\nrelative_error = norm(A - A_approx) / norm(A)\nprintln(\"Relative error: \", relative_error)  # Should be ~0.5-0.8 for random matrix","category":"section"},{"location":"quickstart/#Basic-Workflows","page":"Getting Started","title":"Basic Workflows","text":"","category":"section"},{"location":"quickstart/#1.-Randomized-SVD-(Batch-Processing)","page":"Getting Started","title":"1. Randomized SVD (Batch Processing)","text":"Use rsvd() when you have the entire matrix at once:\n\nusing SketchySVD\n\n# Your matrix\nA = randn(5000, 2000)\n\n# Compute top 50 singular triplets\nU, S, V = rsvd(A, 50)\n\n# Optional: use oversampling for better accuracy\nU, S, V = rsvd(A, 50; p=10)  # p=10 extra dimensions\n\n# Optional: use power iterations for better accuracy\nU, S, V = rsvd(A, 50; q=2)   # q=2 power iterations\n\nWhen to use: Static matrices, batch data, when full matrix fits in memory.","category":"section"},{"location":"quickstart/#2.-Sketchy-SVD-(Streaming-Processing)","page":"Getting Started","title":"2. Sketchy SVD (Streaming Processing)","text":"Use Sketchy when data arrives incrementally:\n\nusing SketchySVD\n\n# Initialize sketchy structure\nm, n = 5000, 2000\nr = 50  # Target rank\nsketchy = init_sketchy(m=m, n=n, r=r)\n\n# Process data column by column\nfor j in 1:n\n    x = get_data_column(j)  # Your data source\n    increment!(sketchy, x)\nend\n\n# Finalize to get SVD\nfinalize!(sketchy)\n\nWhen to use: Streaming data, online learning, memory constraints, temporal data.","category":"section"},{"location":"quickstart/#Choosing-Parameters","page":"Getting Started","title":"Choosing Parameters","text":"","category":"section"},{"location":"quickstart/#Target-Rank-r","page":"Getting Started","title":"Target Rank r","text":"The most important parameter is the target rank:\n\n# Low-rank (fast, less accurate)\nsketchy = init_sketchy(m=m, n=n, r=10)\n\n# Medium-rank (balanced)\nsketchy = init_sketchy(m=m, n=n, r=50)\n\n# High-rank (slower, more accurate)\nsketchy = init_sketchy(m=m, n=n, r=200)\n\nGuidelines:\n\nStart with r = 50 for most applications\nUse r geq 2 times true rank for good accuracy\nIncrease r if approximation error is too high","category":"section"},{"location":"quickstart/#Sketch-Dimension-k","page":"Getting Started","title":"Sketch Dimension k","text":"The sketch dimension controls accuracy vs speed trade-off:\n\n# Default: k = 2r + 1\nsketchy = init_sketchy(m=m, n=n, r=r)\n\n# Higher accuracy: k = 3r\nsketchy = init_sketchy(m=m, n=n, r=r, T=3*r)\n\n# Lower memory: k = r + 10\nsketchy = init_sketchy(m=m, n=n, r=r, T=r+10)\n\nGuidelines:\n\nDefault k = 2r+1 works well for most cases\nUse larger k for noisy data\nMinimum: k geq r","category":"section"},{"location":"quickstart/#Reduction-Map-Type","page":"Getting Started","title":"Reduction Map Type","text":"Choose based on matrix size and structure:\n\n# Gaussian (default): best accuracy\nsketchy = init_sketchy(m=m, n=n, r=r, ReduxMap=:gauss)\n\n# Sparse: large matrices (n > 50,000)\nsketchy = init_sketchy(m=m, n=n, r=r, ReduxMap=:sparse)\n\n# SSRFT: structured/periodic data\nsketchy = init_sketchy(m=m, n=n, r=r, ReduxMap=:ssrft)\n\nSee Dimension Reduction Maps for detailed comparison.","category":"section"},{"location":"quickstart/#Common-Use-Cases","page":"Getting Started","title":"Common Use Cases","text":"","category":"section"},{"location":"quickstart/#Case-1:-Large-Dense-Matrix","page":"Getting Started","title":"Case 1: Large Dense Matrix","text":"# Matrix too large for standard SVD\nm, n = 100_000, 50_000\nA = randn(m, n)  # 37 GB of memory!\n\n# Use randomized SVD with sparse map\nU, S, V = rsvd(A, 100; redux=:sparse)\n\n# Or streaming if matrix doesn't fit in memory\nsketchy = init_sketchy(m=m, n=n, r=100, ReduxMap=:sparse)\nfor j in 1:n\n    # Load column from disk/network\n    x = load_column(j)\n    increment!(sketchy, x)\nend\nfinalize!(sketchy)","category":"section"},{"location":"quickstart/#Case-2:-Time-Series-Data","page":"Getting Started","title":"Case 2: Time Series Data","text":"# Process temporal data with forgetting factor\nm, n = 1000, 5000\nr = 30\nλ = 0.99  # Forget old data slowly\n\nsketchy = init_sketchy(m=m, n=n, r=r)\n\nfor t in 1:n\n    x = get_sensor_data(t)\n    increment!(sketchy, x, 1.0, λ)\n    \n    # Optional: get current approximation\n    if t % 100 == 0\n        finalize!(sketchy)\n        U = sketchy.V \n        S = sketchy.Σ\n        V = sketchy.W\n        analyze_current_state(U, S, V)\n    end\nend","category":"section"},{"location":"quickstart/#Case-3:-Image/Video-Processing","page":"Getting Started","title":"Case 3: Image/Video Processing","text":"# Process video frames\nn_frames = 1000\nheight, width = 480, 640\nm = height * width  # Vectorized frames\n\nsketchy = init_sketchy(m=m, n=n_frames, r=50, ReduxMap=:ssrft)\n\nfor t in 1:n_frames\n    frame = read_video_frame(t)\n    x = vec(frame)  # Flatten to vector\n    increment!(sketchy, x)\nend\n\nfinalize!(sketchy)\n\n# U contains spatial patterns\n# V contains temporal patterns","category":"section"},{"location":"quickstart/#Case-4:-Sparse-Data","page":"Getting Started","title":"Case 4: Sparse Data","text":"using SparseArrays\n\n# Sparse matrix (e.g., from sparse sensor data)\nm, n = 10_000, 5_000\nA = sprand(m, n, 0.01)  # 1% density\n\n# Convert to dense for processing (sketches are dense)\nr = 50\nsketchy = init_sketchy(m=m, n=n, r=r)\n\nfor j in 1:n\n    x = Vector(A[:, j])  # Convert sparse column to dense\n    increment!(sketchy, x)\nend\n\nfinalize!(sketchy)","category":"section"},{"location":"quickstart/#Error-Estimation","page":"Getting Started","title":"Error Estimation","text":"Enable error estimation to monitor approximation quality:\n\n# Initialize with error tracking\nsketchy = init_sketchy(m=m, n=n, r=r, ErrorEstimate=true)\n\n# Process data\nfor j in 1:n\n    result = increment!(sketchy, x)\n    \n    # Check estimated error (optional)\n    if !isnothing(result.est_err)\n        println(\"Estimated error at column $j: \", result.est_err)\n    end\nend\n\n# Final error estimate\nest_err, _ = finalize(sketchy)\nprintln(\"Final estimated relative error: \", est_err)","category":"section"},{"location":"quickstart/#Performance-Tips","page":"Getting Started","title":"Performance Tips","text":"","category":"section"},{"location":"quickstart/#1.-Use-BLAS-Threads","page":"Getting Started","title":"1. Use BLAS Threads","text":"Julia automatically uses multi-threaded BLAS:\n\nusing LinearAlgebra\nBLAS.set_num_threads(4)  # Use 4 threads\n\n# Now sketching will use parallel BLAS operations\nsketchy = init_sketchy(m=m, n=n, r=r)","category":"section"},{"location":"quickstart/#2.-Batch-Processing","page":"Getting Started","title":"2. Batch Processing","text":"Process multiple columns at once when possible:\n\nsketchy = init_sketchy(m=m, n=n, r=r)\n\n# Instead of column-by-column\nbatch_size = 50\nfor start in 1:batch_size:n\n    stop = min(start + batch_size - 1, n)\n    B = A[:, start:stop]\n    \n    for j in start:stop\n        increment!(sketchy, B[:, j-start+1])\n    end\nend","category":"section"},{"location":"quickstart/#3.-Choose-Right-Map","page":"Getting Started","title":"3. Choose Right Map","text":"For large n, sparse maps are much faster:\n\n# For n > 50,000\nsketchy = init_sketchy(m=m, n=n, r=r, ReduxMap=:sparse)","category":"section"},{"location":"quickstart/#4.-Preallocate-Buffers","page":"Getting Started","title":"4. Preallocate Buffers","text":"Avoid repeated allocations:\n\nsketchy = init_sketchy(m=m, n=n, r=r\nx_buffer = zeros(m)\n\nfor j in 1:n\n    # Reuse buffer instead of allocating\n    load_column!(x_buffer, j)\n    increment!(sketchy, x_buffer)\nend","category":"section"},{"location":"quickstart/#Troubleshooting","page":"Getting Started","title":"Troubleshooting","text":"","category":"section"},{"location":"quickstart/#Error:-Dimension-Mismatch","page":"Getting Started","title":"Error: Dimension Mismatch","text":"# ERROR: DimensionMismatch\nsketchy = init_sketchy(m=1000, n=500, r=50)\nx = randn(999)  # Wrong size!\nincrement!(sketchy, x)\n\nSolution: Ensure length(x) == m where m is the row dimension.","category":"section"},{"location":"quickstart/#Poor-Approximation-Quality","page":"Getting Started","title":"Poor Approximation Quality","text":"# Relative error too high\nU, S, V = rsvd(A, 20)\nerror = norm(A - U*Diagonal(S)*V') / norm(A)\nprintln(error)  # Too large!\n\nSolutions:\n\nIncrease rank r\nUse oversampling: rsvd(A, r; p=20)\nUse power iterations: rsvd(A, r; q=2)\nIncrease sketch dimension T","category":"section"},{"location":"quickstart/#Memory-Issues","page":"Getting Started","title":"Memory Issues","text":"# OutOfMemoryError with Gaussian map\nsketchy = init_sketchy(m=100_000, n=500_000, r=100)  # Too large!\n\nSolutions:\n\nUse sparse map: ReduxMap=:sparse\nUse SSRFT: ReduxMap=:ssrft\nReduce rank r\nProcess in chunks","category":"section"},{"location":"quickstart/#Slow-Performance","page":"Getting Started","title":"Slow Performance","text":"# Processing is too slow\n@time for j in 1:n\n    increment!(sketchy, x, j)\nend\n\nSolutions:\n\nUse sparse map for large n\nEnable BLAS threading\nReduce sketch dimension T\nUse batch processing","category":"section"},{"location":"quickstart/#Need-Help?","page":"Getting Started","title":"Need Help?","text":"Issues: Report bugs on GitHub Issues\nDiscussions: Ask questions on GitHub Discussions\nDocumentation: Read the full documentation","category":"section"},{"location":"sketching/#Sketching-Algorithms","page":"Sketching Algorithms","title":"Sketching Algorithms","text":"This page provides detailed information about the sketching algorithm implemented in SketchySVD.jl, based on [1].","category":"section"},{"location":"sketching/#Algorithm-Overview","page":"Sketching Algorithms","title":"Algorithm Overview","text":"The SketchySVD algorithm processes a data matrix A = a_1 a_2 ldots a_n column by column, maintaining compressed representations (sketches) that enable efficient SVD computation.","category":"section"},{"location":"sketching/#Main-Algorithm-(Algorithm-4.1-from-[TYUC2019](@cite))","page":"Sketching Algorithms","title":"Main Algorithm (Algorithm 4.1 from [1])","text":"","category":"section"},{"location":"sketching/#Initialization-Phase","page":"Sketching Algorithms","title":"Initialization Phase","text":"Input: Dimensions m n, target rank r, sketching dimensions k s q\n\nInitialize:\n\nRandom test matrices:\nXi in mathbbR^k times m (range test matrix)\nOmega in mathbbR^k times n (corange test matrix)\nPhi in mathbbR^s times m (core test matrix 1)\nPsi in mathbbR^s times n (core test matrix 2)\nTheta in mathbbR^q times m (error test matrix, Gaussian)\nSketch matrices (initialized to zero):\nX in mathbbR^k times n (corange sketch)\nY in mathbbR^m times k (range sketch)\nZ in mathbbR^s times s (core sketch)\nE in mathbbR^q times n (error sketch, optional)","category":"section"},{"location":"sketching/#Streaming-Phase","page":"Sketching Algorithms","title":"Streaming Phase","text":"For each column a_j (j = 1 2 ldots n):\n\n# Update corange sketch\nX[:, j] = Ξ * a_j\n\n# Update range sketch (rank-1 update)\nY = Y + a_j * Ω[j, :]'\n\n# Update core sketch\nz = Φ * a_j\nZ = Z + z * Ψ[j, :]'\n\n# Update error sketch (optional)\nE[:, j] = Θ * a_j\n\nComputational cost per column:\n\nCorange: O(km)\nRange: O(mk_textnnz) where k_textnnz is number of non-zeros in Omegaj\nCore: O(sm + sk_textnnz)\nError: O(qm)","category":"section"},{"location":"sketching/#Finalization-Phase","page":"Sketching Algorithms","title":"Finalization Phase","text":"After all columns processed:\n\nOrthonormalize range sketch:\nQ_Y = qr(Y).Q\nOrthonormalize corange sketch:\nQ_X = qr(X').Q\nForm intermediate matrix:\n# Solve (Φ * Q_Y) * C = Z\ntemp = (Φ * Q_Y) \\ Z\n# Solve C * (Ψ * Q_X)' = temp\nC = temp / (Ψ * Q_X)'\nCompute SVD of core matrix:\nU_c, Σ_c, V_c = svd(C)\nExtract rank-r approximation:\nV = Q_Y * U_c[:, 1:r]\nΣ = Σ_c[1:r]\nW = Q_X * V_c[:, 1:r]\n\nResult: A approx V Sigma W^T","category":"section"},{"location":"sketching/#Optimized-Implementation-Details","page":"Sketching Algorithms","title":"Optimized Implementation Details","text":"","category":"section"},{"location":"sketching/#Memory-Layout","page":"Sketching Algorithms","title":"Memory Layout","text":"SketchySVD uses column-major storage (Julia's default) for optimal performance:\n\nSequential column access in streaming phase\nEfficient BLAS operations for matrix multiplications\nCache-friendly memory access patterns","category":"section"},{"location":"sketching/#Sparse-Matrix-Optimizations","page":"Sketching Algorithms","title":"Sparse Matrix Optimizations","text":"For sparse test matrices (e.g., Omega Psi), rank-1 updates are optimized:\n\n# Instead of: Y = Y + a_j * Ω[j, :]'\n# Use sparse iteration:\nfor (idx, val) in pairs(Ω[j, :])\n    Y[:, idx] += val * a_j\nend\n\nThis avoids materializing dense intermediate results.","category":"section"},{"location":"sketching/#BLAS-Level-3-Optimizations","page":"Sketching Algorithms","title":"BLAS Level 3 Optimizations","text":"For batch updates (dump! operation):\n\n# Optimized batch update using mul!\nmul!(X, Ξ, A, ν, η)  # X = η*X + ν*(Ξ*A)\nmul!(Y, A, Ω', ν, η)  # Y = η*Y + ν*(A*Ω')\n\nThis leverages highly optimized BLAS routines.","category":"section"},{"location":"sketching/#Batch-vs-Incremental-Processing","page":"Sketching Algorithms","title":"Batch vs Incremental Processing","text":"","category":"section"},{"location":"sketching/#Incremental-Mode-(dump_allfalse)","page":"Sketching Algorithms","title":"Incremental Mode (dump_all=false)","text":"Advantages:\n\nConstant memory footprint\nTrue streaming capability\nCan handle data larger than RAM\n\nUse when:\n\nData arrives sequentially\nMemory is limited\nReal-time processing needed","category":"section"},{"location":"sketching/#Batch-Mode-(dump_alltrue)","page":"Sketching Algorithms","title":"Batch Mode (dump_all=true)","text":"Advantages:\n\nFaster due to BLAS Level 3 operations\nBetter cache utilization\nFewer function calls\n\nUse when:\n\nAll data available at once\nMemory is sufficient\nMaximum speed required","category":"section"},{"location":"sketching/#Algorithm-Variants","page":"Sketching Algorithms","title":"Algorithm Variants","text":"","category":"section"},{"location":"sketching/#Standard-Incremental-Update","page":"Sketching Algorithms","title":"Standard Incremental Update","text":"increment!(sketchy, a_j)  # η=1, ν=1","category":"section"},{"location":"sketching/#Exponential-Forgetting","page":"Sketching Algorithms","title":"Exponential Forgetting","text":"increment!(sketchy, a_j, η, ν)  # η < 1\n\nUseful for tracking time-varying subspaces.","category":"section"},{"location":"sketching/#Fixed-Memory-Sketching","page":"Sketching Algorithms","title":"Fixed-Memory Sketching","text":"Using storage budget T:\n\nsketchy = init_sketchy(m=m, n=n, r=r, T=1000)\n\nAutomatically determines optimal k s for given memory constraint.","category":"section"},{"location":"sketching/#Parallelization-Opportunities","page":"Sketching Algorithms","title":"Parallelization Opportunities","text":"While the current implementation is sequential, several operations can be parallelized:\n\nIndependent sketch updates: X and E updates are independent\nBatch processing: Multiple columns can be processed in parallel\nFinalization: QR decompositions can use parallel BLAS","category":"section"},{"location":"sketching/#Numerical-Stability","page":"Sketching Algorithms","title":"Numerical Stability","text":"The algorithm maintains numerical stability through:\n\nOrthonormalization: QR decompositions ensure orthonormal bases\nScaling: Forgetting factors prevent overflow/underflow\nStable linear solves: Uses backslash operator (LU/QR based)\n\nFor ill-conditioned problems, consider:\n\nIncreasing sketching dimensions k s\nUsing SVD instead of QR for orthonormalization\nRegularization techniques","category":"section"},{"location":"sketching/#Error-Analysis","page":"Sketching Algorithms","title":"Error Analysis","text":"Sources of error:\n\nRank truncation: Approximating rank-rho matrix with rank-r (r  rho)\nSketching: Random projection introduces additional error\nNumerical: Finite precision arithmetic\n\nTotal error bound [1]:\n\nA - VSigma W^T_F^2 leq sum_i=r+1^rho sigma_i^2(A) + epsilon_textsketch + epsilon_textnum\n\nThe sketching error epsilon_textsketch decreases with k s. Note that this a crude generalization and more precise bounds are available in the literature. For the SketchySVD algorithm, refer to theory.md.","category":"section"},{"location":"sketching/#Comparison-with-Standard-SVD","page":"Sketching Algorithms","title":"Comparison with Standard SVD","text":"Aspect Standard SVD SketchySVD\nMemory O(mn) O(k(m+n) + s^2)\nTime (batch) O(mn min(mn)) O((k+s)mn)\nTime (stream) Not applicable O(km + sn) per column\nAccuracy Exact Approximate\nOnline No Yes\n\nSketchySVD is advantageous when:\n\nk s ll min(mn)\nStreaming/online processing required\nMemory constrained\nLow-rank structure exists","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides comprehensive examples demonstrating various use cases of SketchySVD.jl.","category":"section"},{"location":"examples/#Example-1:-Basic-Low-Rank-Approximation","page":"Examples","title":"Example 1: Basic Low-Rank Approximation","text":"Compute a low-rank approximation of a random matrix:\n\nusing SketchySVD\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(42)\n\n# Create a low-rank matrix with noise\nm, n, true_rank = 1000, 500, 20\nU_true = randn(m, true_rank)\nU_true = qr(U_true).Q  # Orthonormalize\nV_true = randn(n, true_rank)\nV_true = qr(V_true).Q  # Orthonormalize\nS_true = sort(rand(true_rank), rev=true) .* 100\nA = U_true * Diagonal(S_true) * V_true' + 0.1 * randn(m, n)\n\n# Compute approximation\nr = 30  # Oversample beyond true rank\nU, S, V = rsvd(A, r)\n\n# Evaluate quality\nA_approx = U * Diagonal(S) * V'\nrelative_error = norm(A - A_approx) / norm(A)\nprintln(\"Relative approximation error: $relative_error\")\n\n# Compare singular values\nusing Plots\nplot(S_true, label=\"True\", marker=:circle, ylabel=\"Singular Value\", xlabel=\"Index\")\nplot!(S[1:true_rank], label=\"Estimated\", marker=:square, legend=:topright)\n\nExpected output: Relative error < 1%, singular values closely match.","category":"section"},{"location":"examples/#Example-2:-Image-Compression","page":"Examples","title":"Example 2: Image Compression","text":"Compress an image using low-rank approximation:\n\nusing SketchySVD\nusing TestImages, ImageIO\nusing LinearAlgebra\n\n# Load image (or create synthetic)\nimg = Float64.(Gray.(testimage(\"cameraman\")))\nm, n = size(img)\n\n# Compress with different ranks\nranks = [5, 10, 20, 50, 100]\ncompressed_images = []\n\nfor r in ranks\n    U, S, V = rsvd(img, r)\n    img_approx = U * Diagonal(S) * V'\n    \n    error = norm(img - img_approx) / norm(img)\n    compression_ratio = (m * n) / (r * (m + n + 1))\n    \n    println(\"Rank $r: Error = $(round(error, digits=4)), Compression = $(round(compression_ratio, digits=2))x\")\n    push!(compressed_images, img_approx)\nend\n\n# Visualize results\nusing Plots\nplot(\n    plot(Gray.(img), title=\"Original\"),\n    plot(Gray.(compressed_images[1]), title=\"Rank 5\"),\n    plot(Gray.(compressed_images[3]), title=\"Rank 20\"),\n    plot(Gray.(compressed_images[5]), title=\"Rank 100\"),\n    layout=(2,2)\n)\n\nKey insight: Rank 50-100 often gives good visual quality with 10-20x compression.","category":"section"},{"location":"examples/#Example-3:-Streaming-PCA","page":"Examples","title":"Example 3: Streaming PCA","text":"Perform PCA on streaming data:\n\nusing SketchySVD\nusing Statistics\nusing LinearAlgebra\n\n# Simulate streaming sensor data\nn_sensors = 100\nn_timesteps = 5000\ntrue_components = 5\n\n# Generate synthetic data with 5 principal components\nW = randn(n_sensors, true_components)\nH = randn(true_components, n_timesteps)\ndata = W * H + 0.5 * randn(n_sensors, n_timesteps)\n\n# Normalize (center and scale)\ndata_mean = mean(data, dims=2)\ndata_centered = data .- data_mean\n\n# Streaming PCA with Sketchy\nsketchy = init_sketchy(m=n_sensors, n=n_timesteps, r=20, ReduxMap=:sparse)\n\nfor t in 1:n_timesteps\n    x = data_centered[:, t]\n    increment!(sketchy, x)\n    \n    # Optional: periodic updates\n    if t % 1000 == 0\n        _, _ = finalize!(sketchy)\n        println(\"At t=$t: Top 5 singular values: $(sketchy.Σ[1:5])\")\n    end\nend\n\n# Final PCA\nfinalize!(sketchy)\n\n# Principal components are columns of U\n# Projections onto components are rows of Diagonal(S) * V'\n\n# Analyze variance explained\nS = sketchy.Σ\ntotal_variance = sum(S.^2)\nvariance_explained = cumsum(S.^2) ./ total_variance\n\nusing Plots\nplot(variance_explained[1:20], \n     label=\"Cumulative Variance Explained\",\n     marker=:circle,\n     xlabel=\"Number of Components\",\n     ylabel=\"Fraction of Variance\")\nhline!([0.9, 0.95, 0.99], label=[\"90%\" \"95%\" \"99%\"], linestyle=:dash)\n\nKey insight: Usually 5-10 components explain > 90% of variance in real data.","category":"section"},{"location":"examples/#Example-4:-Video-Background-Subtraction","page":"Examples","title":"Example 4: Video Background Subtraction","text":"Separate foreground objects from video background:\n\nusing SketchySVD\nusing LinearAlgebra\n\n# Simulate video: static background + moving foreground\nn_frames = 200\nheight, width = 64, 64\nm = height * width\n\n# Background: low-rank\nbackground = randn(height, width, 3)\nbackground_flat = zeros(m, n_frames)\nfor t in 1:n_frames\n    background_flat[:, t] = vec(background[:,:,1])  # Static\nend\n\n# Foreground: sparse\nforeground = zeros(m, n_frames)\nfor t in 1:n_frames\n    # Moving object\n    obj_size = 10\n    x_pos = 20 + div(t * width, n_frames)\n    y_pos = 30\n    indices = (y_pos:y_pos+obj_size-1, x_pos:min(x_pos+obj_size-1, width))\n    \n    for i in indices[1], j in indices[2]\n        idx = (j-1) * height + i\n        if idx <= m\n            foreground[idx, t] = 100  # Bright object\n        end\n    end\nend\n\n# Composite video\nvideo = background_flat + foreground\n\n# Robust PCA via sketching\nr_background = 5  # Background is low-rank\nsketchy = init_sketchy(m=m, n=n_frames, r=r_background, ReduxMap=:sparse)\n\nfor t in 1:n_frames\n    increment!(sketchy, video[:, t])\nend\n\nfinalize!(sketchy)\nU = sketchy.V\nS = sketchy.Σ\nV = sketchy.W\nbackground_recovered = U * Diagonal(S) * V'\nforeground_recovered = video - background_recovered\n\n# Visualize results\nusing Plots\nt_show = 100\nplot(\n    heatmap(reshape(video[:, t_show], height, width), title=\"Original Frame\"),\n    heatmap(reshape(background_recovered[:, t_show], height, width), title=\"Background\"),\n    heatmap(reshape(foreground_recovered[:, t_show], height, width), title=\"Foreground\"),\n    layout=(1,3), colorbar=false\n)\n\nKey insight: Low-rank approximation captures static/slow-varying background, residual is foreground.","category":"section"},{"location":"examples/#Example-5:-Temporal-Data-with-Forgetting","page":"Examples","title":"Example 5: Temporal Data with Forgetting","text":"Track evolving patterns in non-stationary time series:\n\nusing SketchySVD\nusing Plots\n\n# Simulate evolving system: frequencies change over time\nn_sensors = 50\nn_timesteps = 2000\nm = n_sensors\n\nfunction generate_evolving_data(t, n_sensors)\n    # Frequency drifts over time\n    freq1 = 0.1 + 0.05 * sin(2π * t / 500)\n    freq2 = 0.2 + 0.03 * cos(2π * t / 700)\n    \n    x = zeros(n_sensors)\n    for i in 1:n_sensors\n        phase = 2π * i / n_sensors\n        x[i] = sin(2π * freq1 * t + phase) + 0.5 * sin(2π * freq2 * t)\n    end\n    \n    return x + 0.1 * randn(n_sensors)\nend\n\n# Track with exponential forgetting\nη = 0.99  # Forget slowly\nsketchy = init_sketchy(m=m, n=n_timesteps, r=10, ErrorEstimate=true)\n\nerrors = Float64[]\ntop_sv = Float64[]\n\nfor t in 1:n_timesteps\n    x = generate_evolving_data(t, n_sensors)\n    result = increment!(sketchy, x, 1.0, η)\n    \n    # Track every 50 steps\n    if t % 50 == 0\n        est_err, _ = finalize!(sketchy)\n        S = sketchy.Σ\n        push!(errors, isnothing(est_err) ? NaN : est_err)\n        push!(top_sv, S[1])\n        \n        println(\"t=$t: Top SV = $(round(S[1], digits=2)), Error = $(isnothing(est_err) ? \"N/A\" : round(est_err, digits=4))\")\n    end\nend\n\n# Plot evolution\nplot(\n    plot(50:50:n_timesteps, top_sv, label=\"Top Singular Value\", ylabel=\"Value\"),\n    plot(50:50:n_timesteps, errors, label=\"Estimated Error\", ylabel=\"Error\"),\n    layout=(2,1), xlabel=\"Time\", legend=:topright\n)\n\nKey insight: Forgetting factor allows tracking of non-stationary dynamics.","category":"section"},{"location":"examples/#Example-6:-Large-Scale-Matrix-with-Sparse-Map","page":"Examples","title":"Example 6: Large-Scale Matrix with Sparse Map","text":"Efficiently handle very large matrices:\n\nusing SketchySVD\nusing LinearAlgebra\nusing Printf\n\n# Very large matrix (simulated - not created in memory)\nm, n = 100_000, 50_000\nr = 100\n\nprintln(\"Matrix size: $m × $n ($(m*n/1e9) billion elements)\")\nprintln(\"Would require $(8*m*n/1e9) GB if stored as Float64\")\n\n# Function to generate columns on-the-fly\nfunction generate_column(j, m, true_rank=50)\n    # Simulate low-rank structure\n    U_col = randn(m) * sin(2π * j / n)  # Smooth variation\n    return U_col + 0.01 * randn(m)\nend\n\n# Use sparse reduction map for efficiency\nsketchy = init_sketchy(m=m, n=n, r=r, ReduxMap=:sparse)\n\n# Process in streaming fashion\nprintln(\"\\nProcessing columns...\")\n@time begin\n    for j in 1:n\n        x = generate_column(j, m)\n        increment!(sketchy, x)\n        \n        if j % 10_000 == 0\n            @printf(\"Progress: %d/%d (%.1f%%)\\n\", j, n, 100*j/n)\n        end\n    end\nend\n\nprintln(\"\\nFinalizing...\")\n@time _, _ = finalize!(sketchy)\n\nprintln(\"\\nTop 10 singular values:\")\nS = sketchy.Σ\nprintln(S[1:10])\n\n# Memory usage (approximate)\nsketchy_size = sizeof(sketchy.Y) + sizeof(sketchy.Z) + sizeof(sketchy.Ω)\nprintln(\"\\nApproximate memory used: $(sketchy_size/1e9) GB\")\n\nKey insight: Sparse maps enable processing matrices thousands of times larger than available RAM.","category":"section"},{"location":"examples/#Example-7:-Comparison-with-Standard-SVD","page":"Examples","title":"Example 7: Comparison with Standard SVD","text":"Compare accuracy and speed of different methods:\n\nusing SketchySVD\nusing LinearAlgebra\nusing BenchmarkTools\nusing Printf\n\n# Create test matrix\nm, n = 2000, 1000\ntrue_rank = 50\nU_true = randn(m, true_rank)\nS_true = exp.(-0.1 * (1:true_rank))  # Exponential decay\nV_true = randn(n, true_rank)\nA = U_true * Diagonal(S_true) * V_true' + 0.01 * randn(m, n)\n\nr = 60  # Target rank\n\n# Method 1: Standard SVD (expensive)\nprintln(\"Standard SVD:\")\n@time U_full, S_full, V_full = svd(A)\nA_full = U_full[:, 1:r] * Diagonal(S_full[1:r]) * V_full[:, 1:r]'\nerr_full = norm(A - A_full) / norm(A)\nprintln(\"Error: $err_full\")\n\n# Method 2: Randomized SVD\nprintln(\"\\nRandomized SVD:\")\n@time U_rsvd, S_rsvd, V_rsvd = rsvd(A, r)\nA_rsvd = U_rsvd * Diagonal(S_rsvd) * V_rsvd'\nerr_rsvd = norm(A - A_rsvd) / norm(A)\nprintln(\"Error: $err_rsvd\")\n\n# Method 3: Randomized SVD with power iterations\nprintln(\"\\nRandomized SVD (q=2):\")\n@time U_rsvd2, S_rsvd2, V_rsvd2 = rsvd(A, r; q=2)\nA_rsvd2 = U_rsvd2 * Diagonal(S_rsvd2) * V_rsvd2'\nerr_rsvd2 = norm(A - A_rsvd2) / norm(A)\nprintln(\"Error: $err_rsvd2\")\n\n# Method 4: Sketchy (streaming)\nprintln(\"\\nSketchy SVD:\")\n@time begin\n    sketchy = init_sketchy(m=m, n=n, r=r, ReduxMap=:sparse)\n    for j in 1:n\n        increment!(sketchy, A[:, j])\n    end\n    finalize!(sketchy)\nend\nU_sketchy = sketchy.V\nS_sketchy = sketchy.Σ\nV_sketchy = sketchy.W\nA_sketchy = U_sketchy * Diagonal(S_sketchy) * V_sketchy'\nerr_sketchy = norm(A - A_sketchy) / norm(A)\nprintln(\"Error: $err_sketchy\")\n\n# Summary\nprintln(\"\\n\" * \"=\"^50)\nprintln(\"Summary:\")\nprintln(\"Method                    Error         Speedup\")\nprintln(\"-\" * \"=\"^50)\n@printf(\"Standard SVD            %.2e       1.0x (baseline)\\n\", err_full)\n@printf(\"Randomized SVD          %.2e       ~10-50x\\n\", err_rsvd)\n@printf(\"Randomized SVD (q=2)    %.2e       ~5-25x\\n\", err_rsvd2)\n@printf(\"Sketchy SVD             %.2e       ~5-20x\\n\", err_sketchy)\n\nKey insight: Randomized methods are 10-50x faster with minimal accuracy loss.","category":"section"},{"location":"examples/#Example-8:-Adaptive-Rank-Selection","page":"Examples","title":"Example 8: Adaptive Rank Selection","text":"Automatically determine the appropriate rank:\n\nusing SketchySVD\nusing LinearAlgebra\n\n# Unknown true rank\nm, n = 1000, 800\nA = rand(m, n)\n\n# Start with high rank estimate\nr_max = 100\nU, S, V = rsvd(A, r_max)\n\n# Find rank where singular values drop\nthreshold = 0.03 * S[1]  # 1% of largest singular value\nr_effective = findfirst(S .< threshold)\n\nif isnothing(r_effective)\n    r_effective = r_max\n    println(\"Need rank > $r_max\")\nelse\n    println(\"Effective rank: $r_effective\")\nend\n\n# Truncate to effective rank\nU = U[:, 1:r_effective]\nS = S[1:r_effective]\nV = V[:, 1:r_effective]\n\n# Verify accuracy\nA_approx = U * Diagonal(S) * V'\nprintln(\"Relative error with rank $r_effective: $(norm(A - A_approx) / norm(A))\")\n\n# Plot singular value decay\nusing Plots\nplot(S, yscale=:log10, label=\"Singular Values\", \n     marker=:circle, ylabel=\"Singular Value (log scale)\", xlabel=\"Index\")\nhline!([threshold], label=\"Threshold\", linestyle=:dash)\n\nKey insight: Adaptive rank selection balances accuracy and efficiency.","category":"section"},{"location":"examples/#Example-9:-Burgers'-Equation-(PDE-Solution)","page":"Examples","title":"Example 9: Burgers' Equation (PDE Solution)","text":"Analyze solutions of a 1D viscous Burgers' equation:\n\nusing SketchySVD\nusing LinearAlgebra\nusing Plots\n\n# Load or generate Burgers' equation data\n# (see scripts/burgers.jl for data generation)\n\n# Assuming data is in scripts/data/\nusing CSV, DataFrames\ndata_path = \"scripts/data/viscous_burgers1d_states.csv\"\n\nif isfile(data_path)\n    df = CSV.read(data_path, DataFrame)\n    snapshots = Matrix(df)  # Each column is a spatial snapshot at one time\n    \n    m, n = size(snapshots)\n    println(\"Loaded $n snapshots of size $m\")\n    \n    # Compute POD (Proper Orthogonal Decomposition) modes\n    r = 20\n    U, S, V = rsvd(snapshots, r)\n    \n    # U columns are POD modes (spatial patterns)\n    # V columns are temporal coefficients\n    \n    # Analyze modal energy\n    modal_energy = S.^2\n    total_energy = sum(modal_energy)\n    energy_fraction = cumsum(modal_energy) ./ total_energy\n    \n    println(\"\\nModal energy distribution:\")\n    for i in 1:min(10, r)\n        @printf(\"Mode %2d: %.2f%% (cumulative: %.2f%%)\\n\", \n                i, 100*modal_energy[i]/total_energy, 100*energy_fraction[i])\n    end\n    \n    # Visualize modes\n    p1 = plot(U[:, 1:4], label=[\"Mode 1\" \"Mode 2\" \"Mode 3\" \"Mode 4\"],\n              xlabel=\"Spatial Position\", ylabel=\"Amplitude\", title=\"POD Modes\")\n    \n    p2 = plot(energy_fraction[1:r], marker=:circle, \n              xlabel=\"Mode Index\", ylabel=\"Cumulative Energy Fraction\",\n              title=\"Energy Spectrum\", legend=false)\n    hline!([0.99], linestyle=:dash, color=:red)\n    \n    plot(p1, p2, layout=(2,1))\nelse\n    println(\"Data file not found. Run scripts/burgers.jl first.\")\nend\n\nKey insight: PDE solutions often have low effective rank; 5-10 modes capture > 99% energy.","category":"section"},{"location":"examples/#Example-10:-Real-Time-Anomaly-Detection","page":"Examples","title":"Example 10: Real-Time Anomaly Detection","text":"Detect anomalies in streaming data:\n\nusing SketchySVD\nusing LinearAlgebra\nusing Statistics\n\n# Simulation parameters\nn_sensors = 50\nn_timesteps = 1000\nbaseline_rank = 5\n\n# Generate normal data\nfunction generate_normal(t, n)\n    # 5 underlying patterns\n    patterns = rand(n, baseline_rank)\n    weights = [sin(2π*t/100), cos(2π*t/100), sin(2π*t/50), \n               cos(2π*t/50), sin(2π*t/200)]\n    return patterns * weights + 0.1 * rand(n)\nend\n\n# Inject anomalies\nanomaly_times = [300, 600, 850]\n\n# Initialize sketchy \nsketchy = init_sketchy(m=n_sensors, n=n_timesteps, r=10)\n\nreconstruction_errors = Float64[]\nis_anomaly = Bool[]\n\nfor t in 1:n_timesteps\n    # Get data\n    x = generate_normal(t, n_sensors)\n    \n    # Inject anomaly\n    if t in anomaly_times\n        x += 1000 * randn(n_sensors)  # Large spike\n    end\n    \n    # Update sketch\n    increment!(sketchy, x, 1.0, 1.0)\n    \n    # Reconstruction-based anomaly detection\n    if t > 50  # Wait for initialization\n        finalize!(sketchy)\n        U = sketchy.V \n        S = sketchy.Σ\n        V = sketchy.W\n        \n        # Project onto top modes\n        r_detect = 5\n        U_trunc = U[:, 1:r_detect]\n        x_proj = U_trunc * (U_trunc' * x)\n        \n        # Reconstruction error\n        error = norm(x - x_proj) / norm(x)\n        push!(reconstruction_errors, error)\n        \n        # Anomaly threshold (3 standard deviations)\n        if t > 100\n            μ = mean(reconstruction_errors[1:end-1])\n            σ = std(reconstruction_errors[1:end-1])\n            push!(is_anomaly, error > μ + 2.5σ || error < μ - 2.5σ)\n        else\n            push!(is_anomaly, false)\n        end\n    end\nend\n\n# Visualize results\nusing Plots\ntimes = 51:n_timesteps\np = plot(times, reconstruction_errors, label=\"Reconstruction Error\", \n         ylabel=\"Error\", xlabel=\"Time\")\nscatter!(anomaly_times, [reconstruction_errors[t-50] for t in anomaly_times if t > 50], \n         color=:red, markersize=8, label=\"True Anomalies\")\n\n# Mark detected anomalies\ndetected = times[is_anomaly]\nif !isempty(detected)\n    scatter!(detected, reconstruction_errors[is_anomaly], \n             color=:orange, marker=:x, markersize=10, \n             markerstrokewidth=5,\n             label=\"Detected\")\nend\n\nplot!(p, legend=:topleft)\n\nKey insight: Anomalies have large reconstruction error when projected onto normal subspace.","category":"section"},{"location":"examples/#More-Examples","page":"Examples","title":"More Examples","text":"For additional examples, see:\n\nscripts/random_matrix.jl: Random matrix benchmarks\nscripts/burgers.jl: PDE analysis workflow\ntest/ directory: Comprehensive test suite with many usage patterns","category":"section"},{"location":"examples/#Tips-for-Your-Application","page":"Examples","title":"Tips for Your Application","text":"Start simple: Begin with rsvd() on a small dataset\nProfile: Use @time and @benchmark to identify bottlenecks\nIterate: Adjust r, T, and reduction map based on results\nValidate: Compare with standard SVD on small test cases\nMonitor: Use error estimation to track quality","category":"section"},{"location":"examples/#Questions?","page":"Examples","title":"Questions?","text":"If you have questions about applying SketchySVD to your specific problem, please open a discussion on GitHub.","category":"section"},{"location":"rsvd/#Randomized-SVD","page":"Randomized SVD","title":"Randomized SVD","text":"The Randomized SVD (rSVD) is a fast algorithm for computing approximate singular value decompositions using randomized sampling techniques [2].","category":"section"},{"location":"rsvd/#Basic-Algorithm","page":"Randomized SVD","title":"Basic Algorithm","text":"Given a matrix A in mathbbR^m times n and target rank k, randomized SVD computes an approximation A approx U Sigma V^T using the following steps:","category":"section"},{"location":"rsvd/#Algorithm-Steps","page":"Randomized SVD","title":"Algorithm Steps","text":"1. Random Sampling\n\nGenerate a random test matrix G in mathbbR^n times ell where ell = k + p (p is oversampling parameter):\n\nG = textrandn(n ell)\n\n2. Range Approximation\n\nForm the sample matrix and orthonormalize:\n\nY = AG quad Q = textorth(Y)\n\nwhere Q in mathbbR^m times ell has orthonormal columns that approximate the range of A.\n\n3. Dimensionality Reduction\n\nProject A onto the low-dimensional subspace:\n\nB = Q^T A in mathbbR^ell times n\n\n4. SVD of Small Matrix\n\nCompute the SVD of the small matrix B:\n\nB = tildeU tildeSigma tildeV^T\n\n5. Recover Approximate SVD\n\nConstruct the approximate SVD of A:\n\nU = QtildeU_1k quad Sigma = tildeSigma_1k1k quad V = tildeV_1k","category":"section"},{"location":"rsvd/#Power-Iterations","page":"Randomized SVD","title":"Power Iterations","text":"For improved accuracy, especially when the singular values decay slowly, power iterations can be applied:\n\nfunction rsvd_with_power(A, k; p=5, q=2)\n    m, n = size(A)\n    ℓ = k + p\n    \n    # Random matrix\n    G = randn(n, ℓ)\n    \n    # Form sample and orthonormalize\n    Y = A * G\n    Q = Matrix(qr!(Y).Q)\n    \n    # Power iterations\n    for j in 1:q\n        # Z = orth(A' * Q)\n        Z = A' * Q\n        Z = Matrix(qr!(Z).Q)\n        \n        # Q = orth(A * Z)\n        Q = A * Z\n        Q = Matrix(qr!(Q).Q)\n    end\n    \n    # Compute SVD\n    B = Q' * A\n    F = svd!(B)\n    \n    # Extract rank-k approximation\n    U = Q * F.U[:, 1:k]\n    S = F.S[1:k]\n    V = F.V[:, 1:k]\n    \n    return U, S, V\nend\n\nEffect: Each power iteration amplifies the dominant singular values by sigma_i^2, improving the approximation.\n\nRecommendation: Use q=1 or q=2 power iterations for most applications.","category":"section"},{"location":"rsvd/#Transpose-Trick","page":"Randomized SVD","title":"Transpose Trick","text":"For tall-thin matrices (m gg n), it's more efficient to work with A^T:\n\nfunction rsvd_transpose(A, k; p=5, q=0)\n    m, n = size(A)\n    @assert m > 5*n \"Use standard rSVD for non-tall matrices\"\n    \n    ℓ = k + p\n    \n    # Work with transpose\n    G = randn(m, ℓ)\n    Y = A' * G\n    Q = Matrix(qr!(Y).Q)\n    \n    # Power iterations (if q > 0)\n    for j in 1:q\n        Z = A * Q\n        Z = Matrix(qr!(Z).Q)\n        Q = A' * Z\n        Q = Matrix(qr!(Q).Q)\n    end\n    \n    # Compute SVD\n    B = (A * Q)'\n    F = svd!(B)\n    \n    # U and V are swapped\n    V = Q * F.U[:, 1:k]\n    S = F.S[1:k]\n    U = F.V[:, 1:k]\n    \n    return U, S, V\nend\n\nSpeedup: Reduces complexity from O(mn^2) to O(m n ell) when ell ll n.","category":"section"},{"location":"rsvd/#Adaptive-Rank-Selection","page":"Randomized SVD","title":"Adaptive Rank Selection","text":"The rsvd_adaptive function automatically determines the effective rank based on singular value decay:\n\nfunction rsvd_adaptive(A, k_max; tol=1e-10, p=5, q=0)\n    # Compute rSVD with k_max\n    U, S, V = rsvd(A, k_max, p=p, q=q)\n    \n    # Find cutoff where S[i] < tol * S[1]\n    cutoff = findfirst(s -> s < tol * S[1], S)\n    \n    if isnothing(cutoff)\n        return U, S, V\n    else\n        k_actual = cutoff - 1\n        return U[:, 1:k_actual], S[1:k_actual], V[:, 1:k_actual]\n    end\nend\n\nUse case: When the effective rank is unknown but you have a tolerance threshold.","category":"section"},{"location":"rsvd/#Random-Matrix-Types","page":"Randomized SVD","title":"Random Matrix Types","text":"SketchySVD supports multiple random matrix types for different trade-offs:","category":"section"},{"location":"rsvd/#1.-Gaussian-(Standard)","page":"Randomized SVD","title":"1. Gaussian (Standard)","text":"G = randn(n, ℓ)\n\nProperties:\n\nDense matrix\nStrong theoretical guarantees\nStandard choice for general matrices\n\nComplexity: O(mnell) per multiplication","category":"section"},{"location":"rsvd/#2.-Rademacher-(1-entries)","page":"Randomized SVD","title":"2. Rademacher (±1 entries)","text":"G = Float64.(rand([-1, 1], n, ℓ))\n\nProperties:\n\nFaster generation than Gaussian\nSimilar performance to Gaussian\nSmaller storage (can use Int8)\n\nComplexity: Same as Gaussian, but faster generation","category":"section"},{"location":"rsvd/#3.-Sparse-Gaussian","page":"Randomized SVD","title":"3. Sparse Gaussian","text":"using SparseArrays\nG = sprandn(n, ℓ, density)\n\nProperties:\n\nMuch faster multiplication for large n\nRequires higher oversampling p\nGood for very large matrices\n\nComplexity: O(mncdot textdensity cdot ell)\n\nRecommendation: Use density approx min(1 fraclog elln)","category":"section"},{"location":"rsvd/#4.-Subsampled-Randomized-Fourier-Transform-(SRFT)","page":"Randomized SVD","title":"4. Subsampled Randomized Fourier Transform (SRFT)","text":"G = srft_matrix(n, ℓ)\n\nProperties:\n\nImplicitly formed (not stored)\nVery fast via FFT\nExcellent for structured matrices\n\nComplexity: O(mn log n) using FFT\n\nStructure:\n\nG = sqrtfracnell cdot R cdot F cdot D\n\nwhere:\n\nD: diagonal with random ±1 entries\nF: DFT matrix (applied via FFT)\nR: row sampling (selects ell rows)","category":"section"},{"location":"rsvd/#Error-Bounds","page":"Randomized SVD","title":"Error Bounds","text":"For a rank-rho matrix approximated by rank-k rSVD:\n\nExpected error [2]:\n\nmathbbEA - USigma V^T_F leq left(1 + frackp-1right)^12 left(sum_i=k+1^rho sigma_i^2right)^12\n\nWith q power iterations [3]:\n\nmathbbEA - USigma V^T_2 leq left (1 + sqrtfrackp-1)sigma_k+1^2q+1 + fracesqrtk+ppleft( sum_j=k+1^min(mn)sigma_j^2(2q+1) right)^12 right^1(2q+1)\n\nKey insights:\n\nError decreases with oversampling p\nPower iterations drastically reduce error\nError bounded by tail singular values","category":"section"},{"location":"rsvd/#Parameter-Selection-Guidelines","page":"Randomized SVD","title":"Parameter Selection Guidelines","text":"","category":"section"},{"location":"rsvd/#Oversampling-Parameter-p","page":"Randomized SVD","title":"Oversampling Parameter p","text":"Value Use Case\np=5 Default, good balance\np=10 High accuracy required\np=0text-2 Speed priority, good spectrum","category":"section"},{"location":"rsvd/#Power-Iterations-q","page":"Randomized SVD","title":"Power Iterations q","text":"Value Use Case\nq=0 Fast approximation, well-conditioned\nq=1 Standard accuracy\nq=2 High accuracy\nqgeq 3 Ill-conditioned matrices","category":"section"},{"location":"rsvd/#Transpose-Trick-2","page":"Randomized SVD","title":"Transpose Trick","text":"Use when: m  5n and k + p  n\n\nSpeedup: Approximately fracm5n times faster","category":"section"},{"location":"rsvd/#Computational-Complexity","page":"Randomized SVD","title":"Computational Complexity","text":"Operation Standard SVD rSVD (no power) rSVD (q power)\nTime O(mnmin(mn)) O(mnell) O(qmnell)\nSpace O(mn) O(mell + nell) Same\n\nWhere ell = k + p ll min(mn).\n\nExample: For m=10000 n=5000 k=50 p=10:\n\nStandard SVD: sim 10^12 operations\nrSVD: sim 3 times 10^9 operations\nSpeedup: ~300×","category":"section"},{"location":"rsvd/#Numerical-Stability","page":"Randomized SVD","title":"Numerical Stability","text":"","category":"section"},{"location":"rsvd/#Orthonormalization","page":"Randomized SVD","title":"Orthonormalization","text":"Use QR factorization for numerical stability:\n\nQ = Matrix(qr!(Y).Q)  # Stable orthonormalization\n\nAvoid: Gram-Schmidt without reorthogonalization (unstable)","category":"section"},{"location":"rsvd/#Precision-Considerations","page":"Randomized SVD","title":"Precision Considerations","text":"For very ill-conditioned matrices:\n\nUse more power iterations\nIncrease oversampling\nConsider double-double precision\nApply regularization","category":"section"},{"location":"rsvd/#Implementation-in-SketchySVD.jl","page":"Randomized SVD","title":"Implementation in SketchySVD.jl","text":"The package provides three main functions:","category":"section"},{"location":"rsvd/#rsvd-Standard-randomized-SVD","page":"Randomized SVD","title":"rsvd - Standard randomized SVD","text":"U, S, V = rsvd(A, k; p=5, q=0, rng=randn, transpose_trick=true)","category":"section"},{"location":"rsvd/#rsvd_adaptive-Adaptive-rank-selection","page":"Randomized SVD","title":"rsvd_adaptive - Adaptive rank selection","text":"U, S, V = rsvd_adaptive(A, k_max; tol=1e-10, p=5, q=0, rng=randn)","category":"section"},{"location":"rsvd/#Random-Matrix-Generators","page":"Randomized SVD","title":"Random Matrix Generators","text":"# Gaussian (default)\nU, S, V = rsvd(A, k, rng=gaussian_rng)\n\n# Rademacher\nU, S, V = rsvd(A, k, rng=rademacher_rng)\n\n# Sparse Gaussian\nU, S, V = rsvd(A, k, rng=sparse_gaussian_rng(0.1))\n\n# SRFT\nU, S, V = rsvd(A, k, rng=srft_rng)\n\n# Sparse redux\nU, S, V = rsvd(A, k, rng=sparse_rng)","category":"section"},{"location":"rsvd/#Comparison:-Batch-rSVD-vs-Streaming-SketchySVD","page":"Randomized SVD","title":"Comparison: Batch rSVD vs Streaming SketchySVD","text":"Aspect rSVD SketchySVD\nInput Full matrix required Streaming columns\nMemory O(mn + mell + nell) O(k(m+n) + s^2)\nSpeed Fast (BLAS Level 3) Moderate (streaming)\nOnline No Yes\nForgetting No Yes (eta nu)\nUse case Batch processing Streaming/online\n\nWhen to use rSVD: All data available, maximum speed, one-time computation\n\nWhen to use SketchySVD: Streaming data, memory constrained, time-varying subspaces","category":"section"},{"location":"paper/#Paper","page":"References","title":"Paper","text":"Below you have a list of publications referenced in this work.\n\nJ. A. Tropp, A. Yurtsever, M. Udell and V. Cevher. Streaming Low-Rank Matrix Approximation with an Application to Scientific Simulation. SIAM Journal on Scientific Computing 41, A2430-A2463 (2019). Accessed on Aug 1, 2024.\n\n\n\nN. Halko, P. G. Martinsson and J. A. Tropp. Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions. SIAM Review 53, 217–288 (2011). Accessed on Dec 20, 2024.\n\n\n\nP.-G. Martinsson. Randomized Methods for Matrix Computations (Feb 2019), arXiv:1607.01649 [math]. Accessed on Dec 20, 2024.\n\n\n\n","category":"section"},{"location":"#SketchySVD.jl","page":"Home","title":"SketchySVD.jl","text":"Fast, memory-efficient streaming SVD for Julia\n\n(Image: Build Status) (Image: Documentation) (Image: License: MIT)\n\nSketchySVD.jl provides state-of-the-art algorithms for computing low-rank approximations of large matrices using randomized sketching techniques. Based on the SIAM paper by Tropp et al. (2019), this package enables:\n\nStreaming SVD: Process data that doesn't fit in memory\nRandomized SVD: 10-50x faster than standard SVD with minimal accuracy loss\nMultiple sketching methods: Gaussian, Sparse, and SSRFT reduction maps\nOnline learning: Track evolving low-rank structure in non-stationary data","category":"section"},{"location":"#Why-SketchySVD?","page":"Home","title":"Why SketchySVD?","text":"","category":"section"},{"location":"#Fast","page":"Home","title":"🚀 Fast","text":"Randomized algorithms are orders of magnitude faster than standard SVD for large matrices:\n\n# Standard SVD: ~30 seconds for 10000×5000 matrix\nU, S, V = svd(A)\n\n# Randomized SVD: ~2 seconds with comparable accuracy\nU, S, V = rsvd(A, 50)","category":"section"},{"location":"#Memory-Efficient","page":"Home","title":"💾 Memory Efficient","text":"Process matrices larger than available RAM by streaming:\n\n# Matrix: 100GB, RAM: 16GB → No problem!\nsketchy = init_sketchy(100_000, 50_000, 100; ReduxMap=:sparse)\nfor j in 1:n\n    x = load_column_from_disk(j)\n    increment!(sketchy, x, j)\nend\nU, S, V = finalize(sketchy)","category":"section"},{"location":"#Accurate","page":"Home","title":"🎯 Accurate","text":"Theoretical guarantees ensure controlled approximation error:\n\n# Error estimation built-in\nsketchy = init_sketchy(m, n, r; estimate_error=true)\n# ... process data ...\nU, S, V, est_err = finalize(sketchy)\nprintln(\"Estimated relative error: $est_err\")","category":"section"},{"location":"#Flexible","page":"Home","title":"⚡ Flexible","text":"Choose the right algorithm for your problem:\n\nGaussian: Best accuracy, general matrices\nSparse: Fastest for large-scale problems\nSSRFT: Optimal for structured/periodic data","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"Install from the Julia package registry:\n\nusing Pkg\nPkg.add(\"SketchySVD\")\n\nOr install the development version:\n\nPkg.add(url=\"https://github.com/username/SketchySVD.jl\")","category":"section"},{"location":"#Your-First-Computation","page":"Home","title":"Your First Computation","text":"Compute a rank-50 approximation in three lines:\n\nusing SketchySVD, LinearAlgebra\n\nA = randn(5000, 2000)  # Your matrix\nU, S, V = rsvd(A, 50)  # Randomized SVD\nprintln(\"Approximation error: \", norm(A - U*Diagonal(S)*V') / norm(A))","category":"section"},{"location":"#Streaming-Large-Data","page":"Home","title":"Streaming Large Data","text":"Process data that doesn't fit in memory:\n\nsketchy = init_sketchy(m, n, r)  # Initialize\nfor j in 1:n\n    x = load_column(j)            # Get data\n    increment!(sketchy, x, j)     # Update sketch\nend\nU, S, V = finalize(sketchy)      # Extract SVD","category":"section"},{"location":"#Documentation-Overview","page":"Home","title":"Documentation Overview","text":"","category":"section"},{"location":"#For-New-Users","page":"Home","title":"For New Users","text":"Getting Started: Installation, basic usage, parameter selection\nExamples: Complete examples for common applications\nAPI Reference: Function documentation","category":"section"},{"location":"#For-Advanced-Users","page":"Home","title":"For Advanced Users","text":"Mathematical Theory: Sketching framework and theoretical guarantees\nSketching Algorithms: Detailed algorithm walkthrough\nDimension Reduction Maps: Comparison of Gaussian, Sparse, SSRFT\nRandomized SVD: Batch processing algorithms","category":"section"},{"location":"#Key-Algorithms","page":"Home","title":"Key Algorithms","text":"","category":"section"},{"location":"#Randomized-SVD-(Batch)","page":"Home","title":"Randomized SVD (Batch)","text":"For matrices that fit in memory, use rsvd():\n\nU, S, V = rsvd(A, r)           # Basic usage\nU, S, V = rsvd(A, r; p=10)     # With oversampling\nU, S, V = rsvd(A, r; q=2)      # With power iterations\nU, S, V = rsvd(A, r; redux=:sparse)  # Sparse sketching\n\nUse when: Full matrix available, batch processing, quick prototyping.","category":"section"},{"location":"#Sketchy-SVD-(Streaming)","page":"Home","title":"Sketchy SVD (Streaming)","text":"For streaming data, use init_sketchy() + increment!() + finalize():\n\nsketchy = init_sketchy(m, n, r; ReduxMap=:sparse, γ=0.99)\nfor j in 1:n\n    increment!(sketchy, x_j, j)\nend\nU, S, V = finalize(sketchy)\n\nUse when: Data arrives incrementally, memory constrained, online learning.","category":"section"},{"location":"#Usage-Examples","page":"Home","title":"Usage Examples","text":"","category":"section"},{"location":"#Example-1:-Random-Matrix-Decomposition","page":"Home","title":"Example 1: Random Matrix Decomposition","text":"See scripts/random_matrix.jl for a complete example of decomposing a random matrix and comparing results with the exact SVD.\n\nusing SketchySVD\nusing LinearAlgebra\nusing Test\n\n# Generate test matrix\nm, n, r = 1000, 500, 50\nA = randn(m, n)\n\n# Initialize and compute\nsketchy = init_sketchy(m=m, n=n, r=r; method=:ssrft)\nfull_increment!(sketchy, A)\nfinalize!(sketchy)\n\n# Compare with exact SVD\nU_exact, Σ_exact, V_exact = svd(A)\nerror = norm(Σ_exact[1:r] - sketchy.Σ) / norm(Σ_exact[1:r])\nprintln(\"Relative error in singular values: \", error)","category":"section"},{"location":"#Example-2:-Viscous-Burgers-Equation","page":"Home","title":"Example 2: Viscous Burgers Equation","text":"See scripts/burgers.jl for an application to fluid dynamics data.\n\nusing SketchySVD\nusing CSV, DataFrames\n\n# Load data\ndata = CSV.read(\"scripts/data/viscous_burgers1d_states.csv\", DataFrame)\nA = Matrix(data)\nm, n = size(A)\n\n# Compute sketchy SVD\nr = 10\nsketchy = init_sketchy(m=m, n=n, r=r; method=:gauss)\nfull_increment!(sketchy, A)\nfinalize!(sketchy)\n\n# Analyze dominant modes\nprintln(\"Top 5 singular values: \", sketchy.Σ[1:5])","category":"section"},{"location":"#Sketching-Methods","page":"Home","title":"Sketching Methods","text":"","category":"section"},{"location":"#Gaussian-Random-Projection-(:gauss)","page":"Home","title":"Gaussian Random Projection (:gauss)","text":"Projects data using Gaussian random matrices. Provides good accuracy with moderate computational cost.","category":"section"},{"location":"#Sparse-Random-Projection-(:sparse)","page":"Home","title":"Sparse Random Projection (:sparse)","text":"Uses sparse random matrices for projection. Faster than Gaussian for very large matrices with similar accuracy.","category":"section"},{"location":"#Subsampled-Randomized-Fourier-Transform-(:ssrft)","page":"Home","title":"Subsampled Randomized Fourier Transform (:ssrft)","text":"Leverages FFT for fast random projections. Most efficient for structured matrices.","category":"section"},{"location":"#Performance-Tips","page":"Home","title":"Performance Tips","text":"Choose appropriate rank: Set r to capture desired accuracy while minimizing computation\nAdjust oversampling: Increase for better accuracy, decrease for speed\nSelect optimal method: \nUse :ssrft for structured/large matrices\nUse :sparse for very large data\nUse :gauss for general matrices\nBatch processing: Use incremental updates for streaming data","category":"section"},{"location":"#Testing","page":"Home","title":"Testing","text":"Run the test suite:\n\nusing Pkg\nPkg.test(\"SketchySVD\")\n\nOr run individual examples:\n\ninclude(\"scripts/random_matrix.jl\")\ninclude(\"scripts/burgers.jl\")","category":"section"},{"location":"#Project-Structure","page":"Home","title":"Project Structure","text":"SketchySVD.jl/\n├── src/\n│   ├── SketchySVD.jl      # Main module\n│   ├── sketchy.jl         # Core data structures\n│   ├── increment.jl       # Incremental update functions\n│   ├── finalize.jl        # Finalization and SVD computation\n│   └── redux/\n│       ├── dimredux.jl    # Dimension reduction interface\n│       ├── gauss.jl       # Gaussian sketching\n│       ├── sparse.jl      # Sparse sketching\n│       └── ssrft.jl       # SSRFT sketching\n├── scripts/\n│   ├── random_matrix.jl   # Example: random matrix SVD\n│   ├── burgers.jl         # Example: Burgers equation\n│   └── data/\n│       └── viscous_burgers1d_states.csv\n└── Project.toml","category":"section"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"Contributions are welcome! Please feel free to submit a Pull Request.","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This project is licensed under the MIT License - see the LICENSE file for details.","category":"section"},{"location":"#References","page":"Home","title":"References","text":"J. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher, “Streaming Low-Rank Matrix Approximation with an Application to Scientific Simulation,” SIAM J. Sci. Comput., vol. 41, no. 4, pp. A2430–A2463, Jan. 2019, doi: 10.1137/18M1201068.\nN. Halko, P. G. Martinsson, and J. A. Tropp, “Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions,” SIAM Rev., vol. 53, no. 2, pp. 217–288, Jan. 2011, doi: 10.1137/090771806.","category":"section"},{"location":"#Contact","page":"Home","title":"Contact","text":"Tomoki Koike (tkoike@gatech.edu)","category":"section"}]
}
